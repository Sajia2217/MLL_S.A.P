# -*- coding: utf-8 -*-
"""2217(B)_ MLL_Sentiment_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13jm-EKEoZOXfS0C0ZRbyULZXJCE-1jrS

# Import Libraries:
This section imports all required Python libraries for data processing, text cleaning, visualization, and building machine learning or deep learning sentiment models. It sets up the environment so later code cells can run properly.
"""

import pandas as pd

import re
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, GRU, Bidirectional, Dense, Dropout
from tensorflow.keras.optimizers import Adam

import os
os.environ["PYTHONHASHSEED"] = "42"
os.environ["TF_DETERMINISTIC_OPS"] = "1"

import random
import numpy as np
import tensorflow as tf

random.seed(42)
np.random.seed(42)
tf.random.set_seed(42)

"""# Drive Dataset & Load Dataset:

This part connects Google Colab to Google Drive to access datasets stored there. Once Drive is mounted, the code loads the sentiment dataset from a CSV file into a Pandas DataFrame for further analysis.
"""

from types import DynamicClassAttribute
from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv("/content/drive/MyDrive/MLL_Project/Sentiment_T.csv")

"""# Details Of Dataset:

This section examines the structure of the dataset. It displays sample rows, summary statistics, and unique values of each column. It helps you understand the data format, distribution, and potential preprocessing requirements.
"""

df.head()

df.describe()

df.nunique()

print("ðŸ”¹ First 5 rows:")
print(df.head())

# View column names (features)
print("\nðŸ”¹ Column names:")
print(df.columns)

# View rows, columns
print("\nðŸ”¹ Dataset shape:")
print(df.shape)

# View data types of each column
print("\nðŸ”¹ Data types:")
print(df.dtypes)

# Check missing values
print("\nðŸ”¹ Missing values:")
print(df.isnull().sum())

# View unique
print("\nðŸ”¹ Unique sentiment labels:")
print(df['sentiment_type'].unique())



"""# Distribution Chart of Sentiments Lables:


"""

import matplotlib.pyplot as plt

sentiment_counts = df['sentiment_type'].value_counts()

plt.figure(figsize=(8, 8))
plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%', startangle=90, colors=plt.cm.Paired.colors)
plt.title('Distribution of Sentiment Labels in the Dataset')
plt.ylabel('')
plt.show()

"""# Pre-Processing;

This section cleans and prepares the text by applying operations such as lowercasing, removing punctuation, removing stopwords, tokenizing, and converting text into a model-ready format. Preprocessing ensures higher accuracy in machine learning models.

"""

import nltk
from nltk.corpus import stopwords
import re

# 1. basic cleanup
def clean_label(s):
    s = str(s).strip()            # remove whitespace
    s = re.sub(r'\s+', ' ', s)   # collapse multiple spaces
    s = s.lower()
    return s

# Download stopwords
try:
    stopwords_set = set(stopwords.words('english'))
except LookupError:
    nltk.download('stopwords')
    stopwords_set = set(stopwords.words('english'))

def clean_text_data(text):
    text = str(text).lower()

    # Remove URLs
    text = re.sub(r'https?://\S+|www\.\S+', '', text)

    # Remove hashtags
    text = re.sub(r'#\w+', '', text)

    # Remove emojis
    emoji_pattern = re.compile(
        "["
        "\U0001F600-\U0001F64F"  # emoticons
        "\U0001F300-\U0001F5FF"  # symbols & pictographs
        "\U0001F680-\U0001F6FF"  # transport & map symbols
        "\U0001F1E0-\U0001F1FF"  # flags (iOS)
        "\U00002702-\U000027B0"
        "\U000024C2-\U0001F251"
        "]+", flags=re.UNICODE
    )
    text = emoji_pattern.sub(r'', text)
    text = re.sub(r'[^\w\s]', '', text) # Keep letters, numbers, underscore, space
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'\s+', ' ', text).strip()

    # Remove stopwords
    words = text.split()
    filtered_words = [word for word in words if word not in stopwords_set]
    text = ' '.join(filtered_words)

    return text

# Apply to 'Text' column
df['cleaned_text'] = df['text'].apply(clean_text_data)
print(df[['text', 'cleaned_text']].head())

df['Sentiment_clean'] = df['sentiment_type'].apply(clean_label)

"""# Mapping Labels"""

# 2. mapping to consolidated labels
mapping = {
    # positives
    'positive': 'positive', 'happy': 'positive', 'happiness': 'positive', 'joy': 'positive',
    'gratitude': 'positive','grateful':'positive','hope':'positive','optimism':'positive',
    'contentment':'positive','serenity':'positive','inspiration':'positive','amusement':'positive',
    'love':'positive','admiration':'positive','affection':'positive','encouragement':'positive',

    # negatives
    'negative': 'negative','sad':'negative','sadness':'negative','grief':'negative','despair':'negative',
    'anger':'anger','frustration':'negative','frustrated':'negative','hate':'negative','resentment':'negative',
    'disgust':'negative','boredom':'negative','loneliness':'negative','regret':'negative','helplessness':'negative',

    # fear
    'fear':'fear','anxiety':'fear','anxious':'fear','intimidation':'fear','apprehensive':'fear','overwhelmed':'fear',

    # neutral
    'neutral':'neutral','indifference':'neutral',

    # other specific mapping examples
    'excited':'positive','excitement':'positive','euphoria':'positive',
    # Add many more synonyms here based on your long list...
}

def map_label(s):
    if s in mapping:
        return mapping[s]
    # heuristic rules
    if 'joy' in s or 'happy' in s or 'happiness' in s or 'bless' in s:
        return 'positive'
    if 'sad' in s or 'grief' in s or 'heartbreak' in s:
        return 'negative'
    if 'anger' in s or 'frustr' in s or 'hate' in s:
        return 'anger'
    if 'fear' in s or 'anxi' in s or 'intimid' in s:
        return 'fear'
    if 'neutral' in s:
        return 'neutral'
    # fallback
    return 'other'

df['label'] = df['Sentiment_clean'].apply(map_label)

print(df['label'].value_counts())

"""# Tokenization and Padding

"""

import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

texts = df['text'].astype(str).tolist()
labels = df['label'].tolist()

# tokenizer
MAX_NUM_WORDS = 20000
MAX_SEQ_LEN = 120

tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token='<OOV>')
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)
X = pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post', truncating='post')

# labels -> integers
le = LabelEncoder()
y = le.fit_transform(labels)
num_classes = len(le.classes_)

# train/val/test split
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.30, stratify=y, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.50, random_state=42)

print("Classes:", le.classes_)
print("Train/Val/Test sizes:", X_train.shape, X_val.shape, X_test.shape)

EMBED_DIM = 128

y_train_cat = to_categorical(y_train, num_classes=num_classes)
y_val_cat = to_categorical(y_val, num_classes=num_classes)

"""# Apply Early Stopping"""

from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from sklearn.utils.class_weight import compute_class_weight
import numpy as np

# class weights
class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
cw = {i: class_weights[i] for i in range(len(class_weights))}

es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
mc = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss')

"""# Base model(ML): Naive Bayes"""

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Vectorize
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(df['text'].fillna(''))
y = df['label'].to_numpy()

from sklearn.model_selection import train_test_split
X_train_nb, X_test_nb, y_train_nb, y_test_nb = train_test_split(X, y, test_size=0.2, random_state=42)

nb_model = MultinomialNB()
nb_model.fit(X_train_nb, y_train_nb)

y_pred_nb = nb_model.predict(X_test_nb)
print("ðŸ”¹ Naive Bayes Accuracy:", accuracy_score(y_test_nb, y_pred_nb))
print("\nðŸ”¹ Classification Report (NB):\n")
print(classification_report(y_test_nb, y_pred_nb))

cm = confusion_matrix(y_test_nb, y_pred_nb)
plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - Naive Bayes")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""# Base model(ML): Logistic Regression Model

"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

# TF-IDF vectorizer
tfidf = TfidfVectorizer(max_features=5000)
X = tfidf.fit_transform(df['text'].fillna(''))
y = df['label']

X_train_lr, X_test_lr, y_train_lr, y_test_lr = train_test_split(X, y, test_size=0.2, random_state=42)
lr_model = LogisticRegression(max_iter=200)
lr_model.fit(X_train_lr, y_train_lr)
y_pred_lr = lr_model.predict(X_test_lr)


print(" Logistic Regression Accuracy:", accuracy_score(y_test_lr, y_pred_lr))
print("\n Classification Report (LR):\n")
print(classification_report(y_test_lr, y_pred_lr))

cm = confusion_matrix(y_test_lr, y_pred_lr)

plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - Logistic Regression")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""# **Apply Deep Learning Models
**

# RNN Model
"""

def build_rnn():
    model = Sequential([
        Embedding(input_dim=MAX_NUM_WORDS, output_dim=EMBED_DIM, input_length=MAX_SEQ_LEN),
        SimpleRNN(128, return_sequences=False),
        Dropout(0.4),
        Dense(64, activation='relu'),
        Dropout(0.3),
        Dense(num_classes, activation='softmax')
    ])
    model.compile(loss='categorical_crossentropy', optimizer=Adam(1e-3), metrics=['accuracy'])
    return model

    # Train RNN
model_rnn = build_rnn()
history_rnn = model_rnn.fit(
    X_train, y_train_cat,
    validation_data=(X_val, y_val_cat),
    epochs=20, batch_size=32,
    class_weight=cw,
    callbacks=[es, mc]
)

from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import numpy as np

def get_metrics(y_true, y_pred):
    acc = accuracy_score(y_true, y_pred)
    precision, recall, f1, _ = precision_recall_fscore_support(
        y_true, y_pred, average="weighted"
    )
    return acc, precision, recall, f1

pred_rnn = np.argmax(model_rnn.predict(X_test), axis=1)
rnn_acc, rnn_precision, rnn_recall, rnn_f1 = get_metrics(y_test, pred_rnn)

print("RNN Model Metrics")
print(f"Accuracy:  {rnn_acc:.4f}")
print(f"Precision: {rnn_precision:.4f}")
print(f"Recall:    {rnn_recall:.4f}")
print(f"F1 Score:  {rnn_f1:.4f}")

# RNN Accuracy & Los

plt.figure(figsize=(10,4))

# Accuracy Curve
plt.subplot(1,2,1)
plt.plot(history_rnn.history['accuracy'])
plt.plot(history_rnn.history['val_accuracy'])
plt.title("RNN Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend(['Train', 'Validation'])

# Loss Curve
plt.subplot(1,2,2)
plt.plot(history_rnn.history['loss'])
plt.plot(history_rnn.history['val_loss'])
plt.title("RNN Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend(['Train', 'Validation'])

plt.show()

"""# LSTM Model"""

def build_lstm():
    model = Sequential([
        Embedding(input_dim=MAX_NUM_WORDS, output_dim=EMBED_DIM, input_length=MAX_SEQ_LEN),
        Bidirectional(LSTM(128, return_sequences=False)),
        Dropout(0.4),
        Dense(64, activation='relu'),
        Dropout(0.3),
        Dense(num_classes, activation='softmax')
    ])
    model.compile(loss='categorical_crossentropy', optimizer=Adam(1e-3), metrics=['accuracy'])
    return model

# Train LSTM
model_lstm = build_lstm()
history_lstm = model_lstm.fit(
    X_train, y_train_cat,
    validation_data=(X_val, y_val_cat),
    epochs=5, batch_size=32,
    class_weight=cw,
    callbacks=[es, mc]
)

from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import numpy as np

def get_metrics(y_true, y_pred):
    acc = accuracy_score(y_true, y_pred)
    precision, recall, f1, _ = precision_recall_fscore_support(
        y_true, y_pred, average="weighted"
    )
    return acc, precision, recall, f1

pred_lstm = np.argmax(model_lstm.predict(X_test), axis=1)
lstm_acc, lstm_precision, lstm_recall, lstm_f1 = get_metrics(y_test, pred_lstm)

print("LSTM Accuracy:", accuracy_score(y_test, pred_lstm))
print("\n Classification Report (LSTM):\n")
print(classification_report(y_test, pred_lstm))

# LSTM Accuracy & Loss

plt.figure(figsize=(10,4))

plt.subplot(1,2,1)
plt.plot(history_lstm.history['accuracy'])
plt.plot(history_lstm.history['val_accuracy'])
plt.title("LSTM Accuracy")
plt.legend(['Train', 'Validation'])

plt.subplot(1,2,2)
plt.plot(history_lstm.history['loss'])
plt.plot(history_lstm.history['val_loss'])
plt.title("LSTM Loss")
plt.legend(['Train', 'Validation'])

plt.show()

"""# GRU Model

"""

def build_gru():
    model = Sequential([
        Embedding(input_dim=MAX_NUM_WORDS, output_dim=EMBED_DIM, input_length=MAX_SEQ_LEN),
        Bidirectional(GRU(128, return_sequences=False)),
        Dropout(0.4),
        Dense(64, activation='relu'),
        Dropout(0.3),
        Dense(num_classes, activation='softmax')
    ])
    model.compile(loss='categorical_crossentropy', optimizer=Adam(1e-3), metrics=['accuracy'])
    return model

# Train GRU
model_gru = build_gru()
history_gru = model_gru.fit(
    X_train, y_train_cat,
    validation_data=(X_val, y_val_cat),
    epochs=5, batch_size=32,
    class_weight=cw,
    callbacks=[es, mc]
)
model_gru.save("sentiment_model.h5")

from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import numpy as np

def get_metrics(y_true, y_pred):
    acc = accuracy_score(y_true, y_pred)
    precision, recall, f1, _ = precision_recall_fscore_support(
        y_true, y_pred, average="weighted"
    )
    return acc, precision, recall, f1

pred_gru = np.argmax(model_gru.predict(X_test), axis=1)
gru_acc, gru_precision, gru_recall, gru_f1 = get_metrics(y_test, pred_gru)

print("GRU Accuracy:", accuracy_score(y_test, pred_gru))
print("\n Classification Report (GRU):\n")
print(classification_report(y_test, pred_gru))

# GRU Accuracy & Loss Curve

plt.figure(figsize=(10,4))

plt.subplot(1,2,1)
plt.plot(history_gru.history['accuracy'])
plt.plot(history_gru.history['val_accuracy'])
plt.title("GRU Accuracy")
plt.legend(['Train', 'Validation'])

plt.subplot(1,2,2)
plt.plot(history_gru.history['loss'])
plt.plot(history_gru.history['val_loss'])
plt.title("GRU Loss")
plt.legend(['Train', 'Validation'])

plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc

def plot_conf_matrix(y_true, y_pred, title):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(7,6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(title)
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.tight_layout()
    plt.show()

# Predictions
pred_rnn = np.argmax(model_rnn.predict(X_test), axis=1)
pred_lstm = np.argmax(model_lstm.predict(X_test), axis=1)
pred_gru = np.argmax(model_gru.predict(X_test), axis=1)

plot_conf_matrix(y_test, pred_rnn, "Confusion Matrix - RNN")
plot_conf_matrix(y_test, pred_lstm, "Confusion Matrix - LSTM")
plot_conf_matrix(y_test, pred_gru, "Confusion Matrix - GRU")

from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import pandas as pd

def get_metrics(y_true, y_pred):
    acc = accuracy_score(y_true, y_pred)
    precision, recall, f1, _ = precision_recall_fscore_support(
        y_true, y_pred, average="weighted"
    )
    return acc, precision, recall, f1

rnn_m = get_metrics(y_test, pred_rnn)
lstm_m = get_metrics(y_test, pred_lstm)
gru_m = get_metrics(y_test, pred_gru)
nb_m = get_metrics(y_test_nb, y_pred_nb) # Metrics for Naive Bayes
lr_m = get_metrics(y_test_lr, y_pred_lr) # Metrics for Logistic Regression

df_metrics = pd.DataFrame({
    "Model": ["RNN", "LSTM", "GRU", "Naive Bayes", "Logistic Regression"],
    "Accuracy": [rnn_m[0], lstm_m[0], gru_m[0], nb_m[0], lr_m[0]],
    "Precision": [rnn_m[1], lstm_m[1], gru_m[1], nb_m[1], lr_m[1]],
    "Recall": [rnn_m[2], lstm_m[2], gru_m[2], nb_m[2], lr_m[2]],
    "F1 Score": [rnn_m[3], lstm_m[3], gru_m[3], nb_m[3], lr_m[3]],
})

print("MODEL PERFORMANCE COMPARISON \n")
print(df_metrics)

rnn_acc = history_rnn.history['val_accuracy'][-1]
lstm_acc = history_lstm.history['val_accuracy'][-1]
gru_acc = history_gru.history['val_accuracy'][-1]

print("ðŸ”¹ Final Validation Accuracy Comparison")
print(f"RNN  Accuracy : {rnn_acc:.4f}")
print(f"LSTM Accuracy : {lstm_acc:.4f}")
print(f"GRU  Accuracy : {gru_acc:.4f}")

import matplotlib.pyplot as plt
accuracies = [rnn_acc, lstm_acc, gru_acc]
models = ['RNN', 'LSTM', 'GRU']

colors = ['skyblue', 'lightcoral', 'skyblue'] # Two colors for three models, repeating the first

plt.figure(figsize=(8,5))
plt.bar(models, accuracies, color=colors)
plt.title("Final Validation Accuracy Comparison")
plt.ylabel("Accuracy")
plt.ylim(0, 1)
plt.grid(axis='y', linestyle='--', alpha=0.4)

for i, acc in enumerate(accuracies):
    plt.text(i, acc + 0.02, f"{acc:.2f}", ha='center')

plt.show()

y_test_cat = to_categorical(y_test, num_classes=num_classes)

def plot_roc_curve(model, label_name):
    y_prob = model.predict(X_test)
    fpr, tpr, _ = roc_curve(y_test_cat.ravel(), y_prob.ravel())
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f"{label_name} (AUC={roc_auc:.3f})")

plt.figure(figsize=(8,6))
plot_roc_curve(model_rnn, "RNN")
plot_roc_curve(model_lstm, "LSTM")
plot_roc_curve(model_gru, "GRU")

plt.plot([0,1],[0,1], "k--")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve - RNN vs LSTM vs GRU")
plt.legend()
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 6))
sns.barplot(x='Model', y='Accuracy', data=df_metrics, palette='viridis')
plt.title('Accuracy Comparison of All Models')
plt.xlabel('Model')
plt.ylabel('Accuracy')
plt.ylim(0, 1) # Accuracy values are between 0 and 1

for index, row in df_metrics.iterrows():
    plt.text(index, row['Accuracy'] + 0.02, f"{row['Accuracy']:.3f}", color='black', ha="center")

plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

def predict_sentiment(text, model):
    text = str(text).strip().lower()
    seq = tokenizer.texts_to_sequences([text])
    padded = pad_sequences(seq, maxlen=MAX_SEQ_LEN, padding='post', truncating='post')
    pred = model.predict(padded)
    label_index = np.argmax(pred)
    return le.inverse_transform([label_index])[0]

user_text = input("Enter your text: ")
prediction = predict_sentiment(user_text, model_gru)

print("\nPredicted Sentiment:", prediction)

"""Deployment

"""

import pickle

with open("tokenizer.pkl", "wb") as f:
    pickle.dump(tokenizer, f)